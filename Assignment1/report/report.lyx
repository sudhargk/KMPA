#LyX 2.0 created this file. For more info see http://www.lyx.org/
\lyxformat 413
\begin_document
\begin_header
\textclass article
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman default
\font_sans default
\font_typewriter default
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100

\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\float_placement bh
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry true
\use_amsmath 1
\use_esint 1
\use_mhchem 1
\use_mathdots 1
\cite_engine basic
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 1in
\topmargin 1in
\rightmargin 1in
\bottommargin 1in
\secnumdepth 3
\tocdepth 3
\paragraph_separation skip
\defskip smallskip
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
Kernel Methods and Pattern Analysis
\begin_inset Newline newline
\end_inset

Assignment 1
\end_layout

\begin_layout Author
<<<<<<< HEAD
Parth Joshi (CS09B051) & Sudharshan GK (CS13M050) Umesh Kanoja (CS13M04?)
=======
Parth Joshi (CS09B051) & Sudharshan GK (CS13M050) Umesh Kanoja (CS13M024)
>>>>>>> 77cca7424c1d03cd09657342b67db729b05e9345
\end_layout

\begin_layout Section
Methodology
\end_layout

\begin_layout Enumerate

\series bold
Generating Univariate Data :
\series default
 For the case of univariate data, we used the given function 
\begin_inset Formula $f(x)=\exp(\sin(2\pi x))+\ln(x)$
\end_inset

, adding random gaussian noise with mean 0 and standard deviation 0.3 to
 generate target output values for a given 
\begin_inset Formula $x$
\end_inset

.
 Rather than generating fresh data wherever required, we generated a single
 large dataset initially and saved it to file.
 To obtain datasets of different sizes, we then use a 
\emph on
bootstrapping
\emph default
 mechanism and sample the required number of points with repetition from
 the same dataset.
 The complete dataset generated consists of 10000, 1000 and 1000 samples
 for training, testing and validation respectively.
\end_layout

\begin_layout Enumerate

\series bold
Computing Variance for Gaussian Basis Functions : 
\series default
Our implementation allows the setting of the width for the Gaussian basis
 functions empirically or using the data to compute a measure of the variance.
 For the latter case, we use the average variance of the data as computed
 from the trace of the covariance matrix.
\end_layout

\begin_layout Enumerate

\series bold
Normalization of Data : 
\series default
The datasets were all normalized to the range 
\begin_inset Formula $[0,1]$
\end_inset

 as part of pre-processing since we observed a lot of variance in the range
 of values the different fields of the dataset were taking (particularly
 for the multivariate data) which made the matrices containing the datasets
 badly conditioned.
\end_layout

\begin_layout Enumerate

\series bold
Use of Root Mean Square Error :
\series default
 Instead of directly using the mean squared error (
\begin_inset Formula $MSE$
\end_inset

) for our plots, we have made use of its square root since it gives more
 meaningful ranges for the error value with the error being expressed in
 the same units as the original data.
\end_layout

<<<<<<< HEAD
=======
\begin_layout Standard
\begin_inset Newpage pagebreak
\end_inset


\end_layout

>>>>>>> 77cca7424c1d03cd09657342b67db729b05e9345
\begin_layout Section
Univariate Dataset
\end_layout

\begin_layout Subsection
Plot 1a : Comparison of Plot of Aproximated Function vs Model output for
 Different Complexity and Regularization
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\noindent
\align center
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Plot1a-1"

\end_inset

Plot for N=9
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename figures/Plot1(N=9).jpg
<<<<<<< HEAD
	scale 45
=======
	scale 35
>>>>>>> 77cca7424c1d03cd09657342b67db729b05e9345
	groupId plot1

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\noindent
\align center
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Plot1a-2"

\end_inset

Plot for N=20
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename figures/Plot1(N=20).jpg
<<<<<<< HEAD
	scale 45
=======
	scale 35
>>>>>>> 77cca7424c1d03cd09657342b67db729b05e9345
	groupId plot1

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

<<<<<<< HEAD
\begin_layout Standard
\begin_inset Newpage clearpage
\end_inset


\end_layout

=======
>>>>>>> 77cca7424c1d03cd09657342b67db729b05e9345
\begin_layout Paragraph*
Observations
\end_layout

\begin_layout Itemize
For the smaller number of data points in Fig 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Plot1a-1"

\end_inset

 with no regularization, the 1 degree polynomial gives a poor fit, the 3
 degree polynomial gives a good fit while the degree 8 polynomial gives
 a perfect fit to the training data.
 However, it is evident that the degree 3 polynomial fits the function 
\begin_inset Formula $f(x)$
\end_inset

 to be approximated much better than the higher degree one.
\end_layout

\begin_layout Itemize
After we add a regularization term, the higher degree polynomial gives a
 better approximation to 
\begin_inset Formula $f(x).$
\end_inset


\end_layout

\begin_layout Itemize
When we increase the size of the training dataset in Fig 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Plot1a-2"

\end_inset

, the higher degree polynomial produces an excellent approximation to 
\begin_inset Formula $f(x)$
\end_inset

 as compared to those of lower degree.
\end_layout

\begin_layout Itemize
Again, regularization helps improve the approximation but the effect is
 not as pronounced as in the earlier case.
\end_layout

\begin_layout Paragraph
Inferences
\end_layout

\begin_layout Itemize
As we increase the complexity of the model (the degree of the polynomial),
 the capacity of the model to describe the features of 
\begin_inset Formula $f(x)$
\end_inset

 improves but in the absence of sufficient data, highly complex models may
 get finely tuned to the random noise in the data, resulting in overfitting.
\end_layout

\begin_layout Itemize
Ridge regression using a regularization term helps to overcome the overfitting
 problem if the number of data points available is low.
 It can be observed that the regularization parameter is a means to control
 the effective model complexity.
\end_layout

\begin_layout Itemize
The overfitting problem does not arise if we have sufficient number of training
 examples.
\end_layout

\begin_layout Itemize
The regularization parameter is a tool to control model complexity.
 However, when the model itself is very simple (very few basis functions)
 the effect of the regularization parameter is negligible.
 
\end_layout

<<<<<<< HEAD
=======
\begin_layout Standard
\begin_inset Newpage clearpage
\end_inset


\end_layout

>>>>>>> 77cca7424c1d03cd09657342b67db729b05e9345
\begin_layout Subsection
Plot 1b : Comparison Plot of Bias and Variance for multiple training sets
 for Different Complexity and Regularization Parameter values
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Plot12a"

\end_inset

Varying ln(
\begin_inset Formula $\lambda$
\end_inset

) keeping basis dimension fixed
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\noindent
\align center
\begin_inset Float figure
wide false
sideways false
<<<<<<< HEAD
status collapsed
=======
status open
>>>>>>> 77cca7424c1d03cd09657342b67db729b05e9345

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Plot12a-1"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename figures/Plot1(B=9)ab.jpg
<<<<<<< HEAD
	scale 35
=======
	scale 30
>>>>>>> 77cca7424c1d03cd09657342b67db729b05e9345
	groupId plot1_2

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\noindent
\align center
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename figures/Plot1(B=9)abc.jpg
<<<<<<< HEAD
	scale 35
=======
	scale 30
>>>>>>> 77cca7424c1d03cd09657342b67db729b05e9345
	groupId plot1_2

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\noindent
\align center
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename figures/Plot1(B=9)b.jpg
<<<<<<< HEAD
	scale 35
=======
	scale 30
>>>>>>> 77cca7424c1d03cd09657342b67db729b05e9345
	groupId plot1_2

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Plot12b"

\end_inset

Varying basis keeping ln(
\begin_inset Formula $\lambda$
\end_inset

) fixed
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\noindent
\align center
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename figures/Plot1(B=4)L.jpg
<<<<<<< HEAD
	scale 35
=======
	scale 30
>>>>>>> 77cca7424c1d03cd09657342b67db729b05e9345
	groupId plot1_2

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\noindent
\align center
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename figures/Plot1(B=15)L.jpg
<<<<<<< HEAD
	scale 35
=======
	scale 30
>>>>>>> 77cca7424c1d03cd09657342b67db729b05e9345
	groupId plot1_2

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

<<<<<<< HEAD
\begin_layout Standard
\begin_inset Newpage clearpage
\end_inset


\end_layout

=======
>>>>>>> 77cca7424c1d03cd09657342b67db729b05e9345
\begin_layout Paragraph*
Observations
\end_layout

\begin_layout Itemize
The left hand plots show the estimated output functions for 20 different
 datasets.
 The plots serve to illustrate the variance in the model and its sensitivity
 to the choice of dataset.
\end_layout

\begin_layout Itemize
The right hand plots show the average of the estimated output obtained from
 100 different datasets along with the true function 
\begin_inset Formula $f(x)$
\end_inset

.
 The deviation of the average from 
\begin_inset Formula $f(x)$
\end_inset

 depicts the bias in the model.
\end_layout

\begin_layout Itemize
It is evident from Fig 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Plot12a"

\end_inset

 that as we increase the value of 
\begin_inset Formula $\lambda$
\end_inset

 from 
\begin_inset Formula $\exp(-20)$
\end_inset

 through 
\begin_inset Formula $\exp(-10)$
\end_inset

 to 1 for a fixed number of bases, the variance steadily decreases but the
 bias increases.
\end_layout

\begin_layout Itemize
Conversely, we can see from Fig 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Plot12b"

\end_inset

 that as the number of basis functions is increased for a given 
\begin_inset Formula $\lambda$
\end_inset

, the variance increases while the bias reduces.
\end_layout

\begin_layout Paragraph
Inference
\end_layout

\begin_layout Itemize
There is a trade-off between the bias and the variance as the effective
 model complexity varies.
\end_layout

\begin_layout Itemize
If the regularization parameter 
\begin_inset Formula $\lambda$
\end_inset

 is made too large as in the case of Fig 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Plot12a-1"

\end_inset

, it becomes the dominating factor and even a high model complexity of a
 degree 9 polynomial is unable to prevent severe underfitting of the data.
\end_layout

<<<<<<< HEAD
=======
\begin_layout Standard
\begin_inset Newpage clearpage
\end_inset


\end_layout

>>>>>>> 77cca7424c1d03cd09657342b67db729b05e9345
\begin_layout Subsection
Plot 2: Plot of the squared bias, variance, average loss and the error on
 validation data with varying effective model complexity
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\noindent
\align center
\begin_inset Graphics
	filename figures/PLot2BVLEvsLnLambda.jpg
<<<<<<< HEAD
=======
	scale 35
>>>>>>> 77cca7424c1d03cd09657342b67db729b05e9345

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\noindent
\align center
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename figures/PLot2BVLEvsBasis.jpg
<<<<<<< HEAD
	scale 35
=======
	scale 30
>>>>>>> 77cca7424c1d03cd09657342b67db729b05e9345
	groupId plot1_2

\end_inset


\end_layout

\end_inset


\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename figures/PLot2BVLEvsBasis(ln15).jpg
<<<<<<< HEAD
	scale 35
=======
	scale 30
>>>>>>> 77cca7424c1d03cd09657342b67db729b05e9345
	groupId plot1_2

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

<<<<<<< HEAD
\begin_layout Standard
\begin_inset Newpage clearpage
\end_inset


\end_layout

=======
>>>>>>> 77cca7424c1d03cd09657342b67db729b05e9345
\begin_layout Paragraph*
Observations
\end_layout

\begin_layout Itemize
For a given number of basis functions, the variance decreases with increasing
 
\begin_inset Formula $\lambda$
\end_inset

 while the bias increases.
\end_layout

\begin_layout Itemize
The error on validation data and the average loss always lie above the bias
 and variance curves.
\end_layout

\begin_layout Itemize
As the number of basis functions increases, the value of 
\begin_inset Formula $\ln(\lambda)$
\end_inset

 at which the bias and variance balance each other i.e.
 the best model complexity tends to increase.
\end_layout

\begin_layout Itemize
Keeping the value of 
\begin_inset Formula $\lambda$
\end_inset

 fixed, we can see that the bias decreases while the variance increases
 with increase in the dimensionality of the basis.
\end_layout

\begin_layout Itemize
The point where the bias and variance curves intersect in this case also
 tends to move towards the right as the value of 
\begin_inset Formula $\lambda$
\end_inset

 increases.
\end_layout

\begin_layout Paragraph*
Inferences
\end_layout

\begin_layout Itemize
It is worth noting that the number of basis functions and the regularization
 parameter work in tandem to determine the effective complexity of the model.
 The size of the training dataset being the same, higher dimensionality
 of basis increases complexity while higher 
\begin_inset Formula $\lambda$
\end_inset

 tends to reduce complexity.
\end_layout

\begin_layout Itemize
Thus if we increase the number of basis functions, in order to avoid increasing
 the variance of the model and making it sensitive to the choice of training
 examples, we must increase the value of 
\begin_inset Formula $\lambda$
\end_inset

 as reflected in the shift of bias-variance intersection point.
\end_layout

\begin_layout Itemize
Similarly if we increase the value of 
\begin_inset Formula $\lambda$
\end_inset

, then, all other things being equal, we should also increase the number
 of basis functions in our model to avoid over-regularization which will
 increase the bias inherent in the model.
\end_layout

<<<<<<< HEAD
=======
\begin_layout Standard
\begin_inset Newpage clearpage
\end_inset


\end_layout

>>>>>>> 77cca7424c1d03cd09657342b67db729b05e9345
\begin_layout Subsection
Plot 3 : Plot of root mean squared error (
\begin_inset Formula $E_{RMS}$
\end_inset

) on training, validation and test data for different model complexities
 and 
\begin_inset Formula $\lambda$
\end_inset

 values
\end_layout

<<<<<<< HEAD
\begin_layout Paragraph*
Observations
\end_layout

\begin_layout Paragraph*
Inferences
\end_layout

\begin_layout Subsection
Plot 4 : Plot of model and target outputs for the training, validation and
 test data
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status collapsed
=======
\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\noindent
\align center
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Plot3uni-1-2"

\end_inset


\begin_inset Formula $E_{RMS}$
\end_inset

 vs 
\begin_inset Formula $\ln(\lambda)$
\end_inset

 for number of bases = 50
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename figures/plot3_uni(b=50,N=500)err_vs_b.jpg
	scale 30
	groupId plot1_2

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\noindent
\align center
\begin_inset Float figure
wide false
sideways false
status open
>>>>>>> 77cca7424c1d03cd09657342b67db729b05e9345

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
<<<<<<< HEAD
Number of basis functions = 50, 
\begin_inset Formula $\ln(\lambda)=-25$
=======
\begin_inset CommandInset label
LatexCommand label
name "fig:Plot3uni-1-1"

\end_inset


\begin_inset Formula $E_{RMS}$
\end_inset

 vs number of bases for 
\begin_inset Formula $\ln(\lambda)=-Inf$
>>>>>>> 77cca7424c1d03cd09657342b67db729b05e9345
\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
<<<<<<< HEAD
	filename figures/Plot4_uni(B=50,l=-25).jpg
=======
	filename figures/plot3_uni(l=-Inf,N=50)err_vs_b.jpg
	scale 30
	groupId plot1_2

\end_inset


\end_layout
>>>>>>> 77cca7424c1d03cd09657342b67db729b05e9345

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Paragraph*
<<<<<<< HEAD
Observation
\end_layout

\begin_layout Itemize
All three datasets 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
mdash
=======
Observations
\end_layout

\begin_layout Itemize
As we increase the number of basis functions, the error on training data
 decreases while that on test and validation data increases.
 The minimum test / validation error occurs with 
\begin_inset Formula $\approx5$
\end_inset

 bases.
\end_layout

\begin_layout Itemize
The initial training error for the second plot is very small since a large
 number of basis functions is chosen to describe the trend with 
\begin_inset Formula $\ln(\lambda)$
\end_inset

.
 This results in overfitting.
 The error then shows an increasing trend.
\end_layout

\begin_layout Itemize
The validation error initially decreases gradually till the value of 
\begin_inset Formula $\ln(\lambda)\approx-8$
\end_inset

.
 Beyond this, it sharply increases.
\end_layout

\begin_layout Itemize
The number of data points used for the second plot is much higher than the
 first because we use a larger number of basis functions to get a meaningful
 plot.
\end_layout

\begin_layout Paragraph
Inferences
\end_layout

\begin_layout Itemize
The trend of training, test and validation error is as expected with change
 in the number of bases.
 The training error keeps declining as the model becomes more tuned to the
 data while the test and validation error increases after a point.
\end_layout

\begin_layout Itemize
For very large values of 
\begin_inset Formula $\lambda$
\end_inset

, the error increases since the model complexity effectively becomes very
 low.
\end_layout

\begin_layout Itemize
The validation error displays a minima at 
\begin_inset Formula $\ln(\lambda)\approx-8$
\end_inset

.
 However, the test data at this point is much higher.
 Hence a lower value of the regularizaton parameter would be a better choice.
\end_layout

\begin_layout Standard
\begin_inset Newpage clearpage
\end_inset


\end_layout

\begin_layout Subsection
Plot 4 : Plot of model and target outputs for the training, validation and
 test data
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
Number of basis functions = 50, 
\begin_inset Formula $\ln(\lambda)=-25$
\end_inset


>>>>>>> 77cca7424c1d03cd09657342b67db729b05e9345
\end_layout

\end_inset

<<<<<<< HEAD
 training, test and validation 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
mdash
=======

\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename figures/Plot4_uni(B=50,l=-25).jpg
	scale 35

\end_inset


>>>>>>> 77cca7424c1d03cd09657342b67db729b05e9345
\end_layout

\end_inset

<<<<<<< HEAD
 are well-explained by the model, even for points close to 0 and 1, the
 limits of the range of values.
=======

\end_layout

\begin_layout Paragraph*
Observation
\end_layout

\begin_layout Itemize
All three datasets training, test and validation are well-explained by the
 model, even for points close to 0 and 1, the limits of the range of values.
>>>>>>> 77cca7424c1d03cd09657342b67db729b05e9345
\end_layout

\begin_layout Paragraph*
Inferences
\end_layout

\begin_layout Itemize
Since we have chosen a large number of points for training, we have the
 luxury of setting our polynomial approximator to be of high degree (50
 in this case) even without the need for regularization.
\end_layout

\begin_layout Itemize
The high model complexity ensures that even the points at the edges of the
 range are well-explained by the model (which will usually not be the case).
\end_layout

<<<<<<< HEAD
=======
\begin_layout Standard
\begin_inset Newpage clearpage
\end_inset


\end_layout

>>>>>>> 77cca7424c1d03cd09657342b67db729b05e9345
\begin_layout Subsection
Plot 5 : Scatter plot of target output vs model output for training, validation
 and test data
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\noindent
\align center
\begin_inset Graphics
	filename figures/Plot5a.jpg
<<<<<<< HEAD
=======
	scale 35
>>>>>>> 77cca7424c1d03cd09657342b67db729b05e9345

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Paragraph*
Observation
\end_layout

\begin_layout Itemize
The model output gives a very good correlation with the target output with
 nearly all the plot points lying along the line 
\begin_inset Formula $y=x$
\end_inset

.
\end_layout

\begin_layout Itemize
It can be seen that the density of points in the the range 
\begin_inset Formula $[0,2]$
\end_inset

 is much greater than that elsewhere on the plot.
\end_layout

\begin_layout Itemize
Only in the south-west corner of the graph does the correlation deviate
 from the ideal position.
\end_layout

\begin_layout Paragraph*
Inferences
\end_layout

\begin_layout Itemize
For the univariate dataset, polynomial basis functions can be used to obtain
 a very close approximation to the original function 
\begin_inset Formula $f(x)$
\end_inset

.
\end_layout

\begin_layout Itemize
The points of low correlation are located at the edge of the graph.
 The points of low correlation are outliers due to noise in the data.
\end_layout

<<<<<<< HEAD
=======
\begin_layout Standard
\begin_inset Newpage clearpage
\end_inset


\end_layout

>>>>>>> 77cca7424c1d03cd09657342b67db729b05e9345
\begin_layout Section
Bivariate Data
\end_layout

\begin_layout Subsection
Plot 3 : Plot of root mean squared error (
\begin_inset Formula $E_{RMS}$
\end_inset

) on training, validation and test data for different model complexities
 and 
\begin_inset Formula $\lambda$
\end_inset

 values
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
<<<<<<< HEAD

\end_layout

\begin_layout Plain Layout
=======
>>>>>>> 77cca7424c1d03cd09657342b67db729b05e9345
\begin_inset Caption

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\noindent
\align center
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Plot3bi-1"

\end_inset


\begin_inset Formula $E_{RMS}$
\end_inset

 vs 
\begin_inset Formula $\ln(\lambda)$
\end_inset

 for number of bases = 9
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\noindent
\align center
\begin_inset Graphics
	filename figures/plot3_bi(b=9,var=0.1)err_vs_l.jpg
<<<<<<< HEAD
=======
	scale 35
>>>>>>> 77cca7424c1d03cd09657342b67db729b05e9345
	groupId plot3

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\noindent
\align center
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Plot3bi-2"

\end_inset


\begin_inset Formula $E_{RMS}$
\end_inset

 vs number of bases for 
\begin_inset Formula $\ln(\lambda)=-5$
\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\noindent
\align center
\begin_inset Graphics
	filename figures/plot3_bi(l=-5,var=1)err_vs_b.jpg
<<<<<<< HEAD
=======
	scale 35
>>>>>>> 77cca7424c1d03cd09657342b67db729b05e9345
	groupId plot3

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Paragraph
Observations
\end_layout

\begin_layout Itemize
For a given number of basis functions, the root mean square error on the
 test and validation data decreases with increase in the regularization
 parameter.
 With bases = 9, the test and validation error achieve a minimum value at
 
\begin_inset Formula $\approx\ln(\lambda)=-2$
\end_inset

.
\end_layout

\begin_layout Itemize
Beyond 
\begin_inset Formula $\ln(\lambda)=-2$
\end_inset

, the test and validation error increase once again.
\end_layout

\begin_layout Itemize
There is a gradual increase in the error on the training data as the value
 of 
\begin_inset Formula $\lambda$
\end_inset

 is increased.
\end_layout

\begin_layout Itemize
With a fixed 
\begin_inset Formula $\ln(\lambda)=-5$
\end_inset

, increasing the number of basis functions causes the error on the test
 and validation data to decrease initially until a minimum is reached when
 the number of basis functions is approximately 5.
\end_layout

\begin_layout Itemize
Beyond this point, the test and validation error begin a gradual ascent.
\end_layout

\begin_layout Itemize
The train error decreases with increasing number of basis functions.
\end_layout

\begin_layout Paragraph
Inferences
\end_layout

\begin_layout Itemize
The point 
\begin_inset Formula $\ln(\lambda)=2$
\end_inset

 represents an optimal trade-off between bias and variance where the generalizat
ion ability of the model over unknown data is ideal.
\end_layout

\begin_layout Itemize
With increasing 
\begin_inset Formula $\lambda$
\end_inset

, over-regularization compromises the ability of the model to explain the
 data.
 This also explains the gradual rise in the error on the train data.
\end_layout

\begin_layout Itemize
Similarly, the error minimum when number of basis functions is 5 corresponds
 to an ideal balance between bias and variance of the model.
\end_layout

\begin_layout Itemize
Increasing the number of basis functions keeping the number of training
 examples, 
\begin_inset Formula $\lambda$
\end_inset

 and other parameters constant causes the model to overfit the data.
 Thus, the training error keeps decreasing while the test and validation
 error keep increasing.
\end_layout

<<<<<<< HEAD
=======
\begin_layout Standard
\begin_inset Newpage clearpage
\end_inset

`
\end_layout

>>>>>>> 77cca7424c1d03cd09657342b67db729b05e9345
\begin_layout Subsection
Plot 4 : Plot of model and target outputs for the training, validation and
 test data
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
Model Output vs Target Output
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\noindent
\align center
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename figures/Plot4(B=10).jpg
	scale 35
	groupId plot4

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\noindent
\align center
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename figures/Plot4(B=10)Test.jpg
	scale 35
	groupId plot4

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\noindent
\align center
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename figures/Plot4(B=10)val.jpg
	scale 35
	groupId plot4

\end_inset


\end_layout

\end_inset


\end_layout

<<<<<<< HEAD
\end_inset

=======
\begin_layout Plain Layout
\noindent
\align center
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename figures/BiVariateSpl.jpg
	scale 35
	groupId plot4

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset

>>>>>>> 77cca7424c1d03cd09657342b67db729b05e9345

\end_layout

\begin_layout Standard
<<<<<<< HEAD
\begin_inset Newpage clearpage
=======
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Plot-sm-var"

\end_inset

Model Output vs Target Output for Very Small Variance
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\noindent
\align center
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename figures/BiVariateSpl.jpg
	scale 35
	groupId plot4

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Plot-sm-var-2"

\end_inset


\begin_inset Formula $E_{RMS}$
\end_inset

 vs Variance
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\noindent
\align center
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename figures/plot3var1.jpg
	scale 35
	groupId plot4

\end_inset


\end_layout

\end_inset


\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename figures/plot3var2.jpg
	scale 35
	groupId plot4

\end_inset


\end_layout

\end_inset


\end_layout

>>>>>>> 77cca7424c1d03cd09657342b67db729b05e9345
\end_inset


\end_layout

\begin_layout Paragraph*
Observation
\end_layout

\begin_layout Itemize
The target outputs (green) seem to indicate that the underlying function
 to be approximated is a Gaussian with random noise added.
 As such the use of Gaussian radial basis functions seems to give a reasonably
 good fit to the data except under certain parameter configurations (see
<<<<<<< HEAD
 Fig ).
=======
 Fig 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Plot-sm-var"

\end_inset

 and 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Plot-sm-var-2"

\end_inset

).
\end_layout

\begin_layout Itemize
It can also be observed that increasing the variance hardly has any effect
 on the error once a minimum threshold has been crossed.
>>>>>>> 77cca7424c1d03cd09657342b67db729b05e9345
\end_layout

\begin_layout Paragraph*
Inference
\end_layout

\begin_layout Itemize
Since the target values come from a bell-shaped curve over the feature vector
 space, the underlying function to be approximated does not have many complex
 topological features that may be missed by the model if there is lack of
 sufficient data.
 In essence, the simplicity of the model makes up for some data insufficiency
 in the sense that the data is reducible to its basic statistics - the mean
 and variance.
 If we have sufficient data to estimate the mean well enough, we can vary
 the empirical variance parameter to get a good enough fit to the data.
 Thus, overfitting to the training examples can in general be avoided relatively
 easily.
\end_layout

<<<<<<< HEAD
=======
\begin_layout Itemize
The simplicity of the model also allows the weight vectors to adjust easily
 in case the variance is increased.
 Therefore, increasing the variance does not really change the error much.
\end_layout

\begin_layout Itemize
At very low variance, one can observe the different Gaussians which are
 fit to the data points, where centroid corresponds to the means identified
 by the k-means algorithm.
\end_layout

\begin_layout Standard
\begin_inset Newpage clearpage
\end_inset


\end_layout

>>>>>>> 77cca7424c1d03cd09657342b67db729b05e9345
\begin_layout Subsection
Plot 5 : Scatter plot of target output vs model output for training, validation
 and test data
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
Number of basis functions = 25, 
\begin_inset Formula $\ln(\lambda)=-5$
\end_inset

, variance = 0.1
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\noindent
\align center
\begin_inset Graphics
	filename figures/Plot5b.jpg
<<<<<<< HEAD
=======
	scale 35
>>>>>>> 77cca7424c1d03cd09657342b67db729b05e9345

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Paragraph*
Observations
\end_layout

\begin_layout Itemize
Like the univariate case, the output of our model seems to give a good enough
 correlation to the target output with most of the points lying in a small
 bounded region about the line 
\begin_inset Formula $y=x$
\end_inset

.
\end_layout

\begin_layout Itemize
Again the correlation worsens at the edges of the graph.
\end_layout

\begin_layout Paragraph*
Inferences
\end_layout

\begin_layout Itemize
The above results are very similar to those obtained for the univariate
 data case.
\end_layout

\begin_layout Itemize
As was evident earlier as well, Gaussian basis functions approximate the
 data very well as the function to be approximated itself seems to be a
 Gaussian.
\end_layout

\begin_layout Itemize
The reason for degradation of model performance at the edge of the curve
 is the same as for the univariate case - the points are outliers.
\end_layout

<<<<<<< HEAD
=======
\begin_layout Standard
\begin_inset Newpage clearpage
\end_inset


\end_layout

>>>>>>> 77cca7424c1d03cd09657342b67db729b05e9345
\begin_layout Section
Multivariate Data
\end_layout

\begin_layout Subsection
Plot 3 : Plot of root mean squared error (
\begin_inset Formula $E_{RMS}$
\end_inset

) on training, validation and test data for different model complexities
 and 
\begin_inset Formula $\lambda$
\end_inset

 values
\end_layout

<<<<<<< HEAD
\begin_layout Paragraph*
Observations
\end_layout

\begin_layout Paragraph*
Inferences
\end_layout

\begin_layout Subsection
Plot 5 : Scatter plot of target output vs model output for training, validation
 and test data
\end_layout

\begin_layout Paragraph*
Observations
\end_layout

\begin_layout Paragraph*
Inferences
\end_layout

\begin_layout Bibliography
\labelwidthstring References
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-1"

\end_inset

Vincent Labatut and Hocine Cherifi,
\emph on
 Accuracy Measures for the Comparison of Classifiers 
\emph default

\begin_inset Flex URL
status collapsed

\begin_layout Plain Layout

http://hal.archives-ouvertes.fr/docs/00/61/13/19/PDF/17_ICIT11_VL.pdf
=======
\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\noindent
\align center
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset Formula $E_{RMS}$
\end_inset

 vs number of bases for 
\begin_inset Formula $\ln(\lambda)=-Inf$
\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\noindent
\align center
\begin_inset Graphics
	filename figures/plot3_mul(l=-inf,var=0.5,b=2,225).jpg
	scale 35
	groupId plot3

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\noindent
\align center
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset Formula $E_{RMS}$
\end_inset

 vs 
\begin_inset Formula $\ln(\lambda)$
\end_inset

 for number of bases = 225
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\noindent
\align center
\begin_inset Graphics
	filename figures/plot3_multi(b=225,N=1500)err_vs_b.jpg
	scale 35
	groupId plot3

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Paragraph
Observations
\end_layout

\begin_layout Itemize
While increasing the number of basis functions causes the error on the test
 and validation data to decrease initially.
\end_layout

\begin_layout Itemize
Beyond certain point the train error goes lower but the test and validation
 error starts increasing.
\end_layout

\begin_layout Itemize
For a given number of basis functions, the root mean square error on the
 test and validation data decreases with increase in the regularization
 parameter.
 With bases = 225, the test and validation error achieve a minimum value
 at 
\begin_inset Formula $\approx\ln(\lambda)=-12$
\end_inset

.
\end_layout

\begin_layout Itemize
Beyond 
\begin_inset Formula $\ln(\lambda)=-12$
\end_inset

, the test and validation error increase once again.
\end_layout

\begin_layout Itemize
There is a gradual decrease in the error on the testing and validation data
 for 
\begin_inset Formula $\ln(\lambda)<-12$
\end_inset

.
\end_layout

\begin_layout Paragraph
Inferences
\end_layout

\begin_layout Itemize
By increasing the number of basis functions keeping the number of training
 examples, and other parameters constant causes the model to overfit the
 data.
 Since the number of data is very high we observe a gradual increase in
 testing and validation error.
\end_layout

\begin_layout Itemize
Since the number if bases is high we observe very gradual decrease in root
 mean squared error as the regularization parameter is increased.
\end_layout

\begin_layout Standard
\begin_inset Newpage clearpage
\end_inset


\end_layout

\begin_layout Subsubsection
Plot 5 : Scatter plot of target output vs model output for training, validation
 and test data
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
Number of basis functions = 225, 
\begin_inset Formula $\ln(\lambda)=-15$
\end_inset

, variance = 0.5
>>>>>>> 77cca7424c1d03cd09657342b67db729b05e9345
\end_layout

\end_inset


\end_layout

<<<<<<< HEAD
\begin_layout Bibliography
\labelwidthstring References
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-2"

\end_inset

Marina Sokolovaa, Guy Lapalme
\emph on
, A systematic analysis of performance measures for classiï¬cation task
\emph default
s 
\begin_inset Flex URL
status collapsed

\begin_layout Plain Layout

http://rali.iro.umontreal.ca/rali/sites/default/files/publis/SokolovaLapalme-JIPM09.
pdf
\end_layout

\end_inset


=======
\begin_layout Plain Layout
\noindent
\align center
\begin_inset Graphics
	filename figures/Plot5c.jpg
	scale 40

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subparagraph*
Observations
\end_layout

\begin_layout Itemize
Like the other cases the output of our model seems to give a good enough
 correlation to the target output with most of the points lying in a small
 bounded region about the line 
\begin_inset Formula $y=x$
\end_inset

.
\end_layout

\begin_layout Itemize
There are few outlier points which are not properly fit by our regression
 model.
\end_layout

\begin_layout Subparagraph*
Inferences
\end_layout

\begin_layout Itemize
The above results are very similar to those obtained for the univariate
 data case.
>>>>>>> 77cca7424c1d03cd09657342b67db729b05e9345
\end_layout

\begin_layout Bibliography
\labelwidthstring References
\begin_inset CommandInset bibitem
LatexCommand bibitem
<<<<<<< HEAD
key "key-1"

\end_inset

General reference - 
\begin_inset Flex URL
status collapsed

\begin_layout Plain Layout

http://en.wikipedia.org
\end_layout

\end_inset


=======
key "key-2"

\end_inset


\emph on
 
\begin_inset Quotes eld
\end_inset

Geo-Spotting: Mining Online Location-based Services for Optimal Retail Store
 Placement
\begin_inset Quotes erd
\end_inset

,Dmytro Karamshuk ,IMT Lucca, Italy
>>>>>>> 77cca7424c1d03cd09657342b67db729b05e9345
\end_layout

\end_body
\end_document
